{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path('checkpoints/')\n",
    "pre_trained_models = ['Salesforce-codet5-base', 'microsoft-codebert-base', 'microsoft-graphcodebert-base']\n",
    "languages = ['go', 'java', 'javascript', 'php', 'python', 'ruby']\n",
    "\n",
    "\n",
    "class TrainedModel():\n",
    "    def __init__(self, model_name, language=None, task=None, experiment=None):\n",
    "        self.model_name = model_name\n",
    "        self.language = language\n",
    "        self.experiment = experiment\n",
    "        self.task = task\n",
    "        if self.task == 'codesearch':\n",
    "            self.model_dir = BASE_PATH / self.task / self.experiment / self.language / self.model_name if experiment \\\n",
    "                is not None else BASE_PATH / self.task / self.language / self.model_name\n",
    "        elif self.task == 'code2test':\n",
    "            self.model_dir = BASE_PATH / self.task / self.experiment  / self.model_name if experiment \\\n",
    "                is not None else BASE_PATH / self.task / self.model_name\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def add_metric(self, name, value):\n",
    "        self.metrics[name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_take_models = []\n",
    "\n",
    "for model_name in pre_trained_models:\n",
    "    for language in languages:\n",
    "        model = TrainedModel(model_name, language, 'codesearch', 'two_takes_step_2')\n",
    "        file_name = [f for f in os.listdir(model.model_dir) if 'metric' in f][0]\n",
    "        results = json.load(open(model.model_dir / file_name, 'r'))\n",
    "        model.add_metric('test', results['test'])\n",
    "        model.add_metric('valid', results['valid'])\n",
    "        second_take_models.append(model)\n",
    "\n",
    "\n",
    "original_models = []\n",
    "\n",
    "for model_name in pre_trained_models:\n",
    "    for language in languages:\n",
    "        model = TrainedModel(model_name, language, 'codesearch')\n",
    "        file_name = [f for f in os.listdir(model.model_dir) if 'metric' in f][0]\n",
    "        results = json.load(open(model.model_dir / file_name, 'r'))\n",
    "        model.add_metric('test', results['test'])\n",
    "        model.add_metric('valid', results['valid'])\n",
    "        original_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft-graphcodebert-base\n",
      "go\n",
      "Original {'test': {'mrr': 0.9090841792222955}, 'valid': {'mrr': 0.9207934242164756}}\n",
      "Second Take {'test': {'mrr': 0.6632587202301052}, 'valid': {'mrr': 0.6963504451710881}}\n",
      "java\n",
      "Original {'test': {'mrr': 0.7862952726995615}, 'valid': {'mrr': 0.8362239207723126}}\n",
      "Second Take {'test': {'mrr': 0.3194628964643299}, 'valid': {'mrr': 0.2949044550921596}}\n",
      "javascript\n",
      "Original {'test': {'mrr': 0.5707852428647274}, 'valid': {'mrr': 0.5560066535657695}}\n",
      "Second Take {'test': {'mrr': 0.29621955066693617}, 'valid': {'mrr': 0.2580100816840683}}\n",
      "php\n",
      "Original {'test': {'mrr': 0.8477583594034569}, 'valid': {'mrr': 0.8441795127758595}}\n",
      "Second Take {'test': {'mrr': 0.0872859045249346}, 'valid': {'mrr': 0.06676916396996284}}\n",
      "python\n",
      "Original {'test': {'mrr': 0.7507660138804423}, 'valid': {'mrr': 0.761146024158046}}\n",
      "Second Take {'test': {'mrr': 0.334576342906081}, 'valid': {'mrr': 0.3413115858586959}}\n",
      "ruby\n",
      "Original {'test': {'mrr': 0.704111010027275}, 'valid': {'mrr': 0.7405717526332077}}\n",
      "Second Take {'test': {'mrr': 0.11135059818225285}, 'valid': {'mrr': 0.14372447430891966}}\n"
     ]
    }
   ],
   "source": [
    "#lang = 'python'\n",
    "ptm = pre_trained_models[2]\n",
    "print(ptm)\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    for model in original_models:\n",
    "        if model.language == lang and model.model_name == ptm:\n",
    "            print('Original', model.metrics)\n",
    "    for model in second_take_models:\n",
    "        if model.language == lang and model.model_name == ptm:\n",
    "            print('Second Take', model.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code2Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_take_models = []\n",
    "for model_name in pre_trained_models:\n",
    "    model = TrainedModel(model_name, task='code2test', experiment='two_takes_step_2')\n",
    "    file_name = [f for f in os.listdir(model.model_dir) if 'codebleu' in f][0]\n",
    "    results = json.load(open(model.model_dir / file_name, 'r'))\n",
    "    model.add_metric('ngram_match', results['ngram_match'])\n",
    "    model.add_metric('weighted_ngram_match', results['weighted_ngram_match'])\n",
    "    model.add_metric('syntax_match', results['syntax_match'])\n",
    "    model.add_metric('dataflow_match', results['dataflow_match'])\n",
    "    model.add_metric('code_bleu_score', results['code_bleu_score'])\n",
    "    second_take_models.append(model)\n",
    "\n",
    "\n",
    "original_models = []\n",
    "for model_name in pre_trained_models:\n",
    "    model = TrainedModel(model_name, task='code2test')\n",
    "    file_name = [f for f in os.listdir(model.model_dir) if 'codebleu' in f][0]\n",
    "    results = json.load(open(model.model_dir / file_name, 'r'))\n",
    "    model.add_metric('ngram_match', results['ngram_match'])\n",
    "    model.add_metric('weighted_ngram_match', results['weighted_ngram_match'])\n",
    "    model.add_metric('syntax_match', results['syntax_match'])\n",
    "    model.add_metric('dataflow_match', results['dataflow_match'])\n",
    "    model.add_metric('code_bleu_score', results['code_bleu_score'])\n",
    "    original_models.append(model)\n",
    "\n",
    "prefix_models = []\n",
    "for model_name in pre_trained_models:\n",
    "    model = TrainedModel(model_name, task='code2test', experiment='prefix')\n",
    "    file_name = [f for f in os.listdir(model.model_dir) if 'codebleu' in f][0]\n",
    "    results = json.load(open(model.model_dir / file_name, 'r'))\n",
    "    model.add_metric('ngram_match', results['ngram_match'])\n",
    "    model.add_metric('weighted_ngram_match', results['weighted_ngram_match'])\n",
    "    model.add_metric('syntax_match', results['syntax_match'])\n",
    "    model.add_metric('dataflow_match', results['dataflow_match'])\n",
    "    model.add_metric('code_bleu_score', results['code_bleu_score'])\n",
    "    prefix_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft-graphcodebert-base\n",
      "Original Model\n",
      "{'ngram_match': 0.07265351143778041, 'weighted_ngram_match': 0.08190162200841648, 'syntax_match': 0.34540589033194047, 'dataflow_match': 0.33440243129395353, 'code_bleu_score': 0.20859086376802272}\n",
      "Prefix Model\n",
      "{'ngram_match': 0.06274945984722662, 'weighted_ngram_match': 0.07056071925530413, 'syntax_match': 0.35411137290732, 'dataflow_match': 0.32765149065028554, 'code_bleu_score': 0.20376826066503406}\n",
      "Second Take Model\n",
      "{'ngram_match': 0.031150174562847473, 'weighted_ngram_match': 0.03715885725011193, 'syntax_match': 0.3210261528619512, 'dataflow_match': 0.33917406460562627, 'code_bleu_score': 0.1821273123201342}\n"
     ]
    }
   ],
   "source": [
    "ptm = pre_trained_models[2]\n",
    "print(ptm)\n",
    "\n",
    "print('Original Model')\n",
    "for model in original_models:\n",
    "    if model.model_name == ptm:\n",
    "        print(model.metrics)\n",
    "print('Prefix Model')\n",
    "for model in prefix_models:\n",
    "    if model.model_name == ptm:\n",
    "        print(model.metrics)\n",
    "print('Second Take Model')\n",
    "for model in second_take_models:\n",
    "    if model.model_name == ptm:\n",
    "        print(model.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('code2test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f49e606c151cf2f9b817a2e95f6976cf9447e6d7661b4d1c6f50925f8bbaa67c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
